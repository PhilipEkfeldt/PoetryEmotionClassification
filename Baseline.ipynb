{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpN8-ceIVfWZ"
   },
   "outputs": [],
   "source": [
    "import modules.text_processing\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from imp import reload\n",
    "from modules.text_processing import read_split_file, BatchWrapper, generate_iterators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, val_path, test_path = read_split_file(\"Data/poems_fixed.csv\",\"tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_iter, val_iter, test_iter, TEXT = generate_iterators(\"split_used_for_baseline/poems_train.csv\", \"split_used_for_baseline/poems_val.csv\", \"split_used_for_baseline/poems_test.csv\", \n",
    "                                                           batch_size = batch_size, device=\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "def evaluate(model, data_iter, batch_size):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    total_acc = 0\n",
    "    val_acc = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors,labels = next(data_iter.__iter__())\n",
    "        output = model(vectors)\n",
    "        _, predicted = torch.topk(output.data, k=2, dim=1)\n",
    "        predictions = torch.zeros(labels.size()).to(\"cuda\")\n",
    "        predictions.scatter_(1, predicted, 1)\n",
    "        val_acc +=  ((predictions == labels) & (labels == 1)).sum().item()\n",
    "        total_acc += 2*labels.size(0)\n",
    "        #print(val_acc)\n",
    "        #print(total_acc)\n",
    "        val_loss += F.kl_div(output.log(), labels)\n",
    "        total +=1\n",
    "    return val_loss / total, val_acc / total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter,\n",
    "                  train_eval_iter, test_iter, verbose=True):\n",
    "    \n",
    "    epoch = 0\n",
    "    total_batches = int(len(training_iter))\n",
    "    dev_accuracies = []\n",
    "    test_accuracies = []\n",
    "    while epoch <= num_epochs:\n",
    "        print(\"Training...\")\n",
    "        for i in range(total_batches):\n",
    "            model.train()\n",
    "            vectors, labels = next(training_iter.__iter__())\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(vectors)\n",
    "            lossy = loss_(output.log(), labels)\n",
    "            lossy.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        print(\"Evaluating training....\")\n",
    "        #train_loss, train_acc = evaluate(model, train_eval_iter, batch_size)\n",
    "        train_loss, train_acc = 0,0\n",
    "        print(\"Evaluating dev...\")\n",
    "        eval_loss, eval_acc = evaluate(model, dev_iter, batch_size)\n",
    "        test_loss, test_acc = evaluate(model, test_iter, batch_size)\n",
    "        dev_accuracies.append(eval_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        if verbose:\n",
    "            print(\"Epoch %i; Loss %f; Train loss: %f; Dev loss %f, Dev acc: %f, Test acc: %f\"  %(epoch, lossy.item(), train_acc, eval_loss, eval_acc, test_acc))\n",
    "        epoch += 1    \n",
    "    best_dev = max(dev_accuracies)\n",
    "    best_test = test_accuracies[np.argmax(dev_accuracies)]\n",
    "    return best_dev, best_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.models\n",
    "reload(modules.models)\n",
    "from modules.models import BiLSTMBaseline\n",
    "emotions = ['anger','anticipation','fear','joy','love','optimism','pess','sad']\n",
    "train_batch = BatchWrapper(train_iter, \"text\", emotions)\n",
    "train_eval_batch = BatchWrapper(train_iter, \"text\", emotions)\n",
    "valid_batch = BatchWrapper(val_iter, \"text\", emotions)\n",
    "test_batch = BatchWrapper(test_iter, \"text\", emotions)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 200\n",
    "v_dim = 20\n",
    "label_size = 8\n",
    "v_dim = 20\n",
    "\n",
    "m = BiLSTMBaseline(vocab_size = len(TEXT.vocab), embedding_dim = embedding_dim, hidden_dim = hidden_dim, label_size=label_size, \n",
    "                   v_dim = v_dim, pretrained_vec=TEXT.vocab.vectors, use_gpu = True, dropout = 0)\n",
    "m.to(\"cuda\")\n",
    "opt = torch.optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), 1e-3)\n",
    "training_loop(model=m, training_iter=train_batch, dev_iter=valid_batch, train_eval_iter = train_eval_batch, test_iter = test_batch, loss_=F.kl_div, optim=opt, num_epochs=100, batch_size = batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import modules.models\n",
    "reload(modules.models)\n",
    "from modules.models import BiLSTMBaseline\n",
    "\n",
    "dropout_min = 0.2\n",
    "dropout_max = 0.8\n",
    "\n",
    "learning_rate_min = 0.0001\n",
    "learning_rate_max = 0.01\n",
    "\n",
    "hidden_dim_min = 50\n",
    "hidden_dim_max = 300\n",
    "\n",
    "v_dim_min = 5\n",
    "v_dim_max = 30\n",
    "emotions = ['anger','anticipation','fear','joy','love','optimism','pess','sad']\n",
    "label_size = 8\n",
    "embedding_dim = 300\n",
    "\n",
    "dropouts = []\n",
    "learning_rates = []\n",
    "hidden_dims = []\n",
    "v_dims = []\n",
    "dev_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(60):\n",
    "    print(\"Iteration: \", i)\n",
    "    train_batch = BatchWrapper(train_iter, \"text\", emotions)\n",
    "    train_eval_batch = BatchWrapper(train_iter, \"text\", emotions)\n",
    "    valid_batch = BatchWrapper(val_iter, \"text\", emotions)\n",
    "    test_batch = BatchWrapper(test_iter, \"text\", emotions)\n",
    "    \n",
    "    dropout = np.random.uniform(dropout_min, dropout_max)\n",
    "    learning_rate = 10**np.random.uniform(np.log10(learning_rate_min), np.log10(learning_rate_max))\n",
    "    hidden_dim = np.random.randint(hidden_dim_min, hidden_dim_max)\n",
    "    v_dim = np.random.randint(v_dim_min, v_dim_max)\n",
    "    \n",
    "    m = BiLSTMBaseline(vocab_size = len(TEXT.vocab), embedding_dim = embedding_dim, hidden_dim = hidden_dim, label_size=label_size, \n",
    "                       v_dim = v_dim, pretrained_vec=TEXT.vocab.vectors, use_gpu = True, dropout = dropout)\n",
    "    m.to(\"cuda\")\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), learning_rate)\n",
    "    dev_acc, test_acc = training_loop(model=m, training_iter=train_batch, dev_iter=valid_batch, train_eval_iter = train_eval_batch, test_iter = test_batch, loss_=F.kl_div, optim=opt, num_epochs=10, batch_size = batch_size)\n",
    "\n",
    "    dropouts.append(dropout)\n",
    "    learning_rates.append(learning_rate)\n",
    "    hidden_dims.append(hidden_dim)\n",
    "    v_dims.append(v_dim)\n",
    "    dev_accuracies.append(dev_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    df = pd.DataFrame({\"droprate\": dropouts, \"learning_rates\": learning_rates, \"hidden_dim\": hidden_dims, \"v_dim\": v_dims, \"dev_accuracy\": dev_accuracies, \"test_accuracy\": test_accuracies})\n",
    "    df.to_csv(\"right.csv\")\n",
    "\n",
    "    \n",
    "df = pd.DataFrame({\"droprate\": dropouts, \"learning_rates\": learning_rates, \"hidden_dim\": hidden_dims, \"v_dim\": v_dims, \"dev_accuracy\": dev_accuracies, \"test_accuracy\": test_accuracies})\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>droprate</th>\n",
       "      <th>learning_rates</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>v_dim</th>\n",
       "      <th>dev_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.216915</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>97</td>\n",
       "      <td>23</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.336879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.637862</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>240</td>\n",
       "      <td>15</td>\n",
       "      <td>0.325893</td>\n",
       "      <td>0.280142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.494909</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>149</td>\n",
       "      <td>28</td>\n",
       "      <td>0.334821</td>\n",
       "      <td>0.336879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.350545</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>264</td>\n",
       "      <td>29</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.297872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.638593</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>166</td>\n",
       "      <td>25</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.280142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>281</td>\n",
       "      <td>19</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>0.368794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.356340</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>231</td>\n",
       "      <td>17</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>0.354610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.532029</td>\n",
       "      <td>0.003339</td>\n",
       "      <td>129</td>\n",
       "      <td>27</td>\n",
       "      <td>0.352679</td>\n",
       "      <td>0.322695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.579366</td>\n",
       "      <td>0.004421</td>\n",
       "      <td>152</td>\n",
       "      <td>6</td>\n",
       "      <td>0.352679</td>\n",
       "      <td>0.368794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.498583</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>211</td>\n",
       "      <td>18</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.297872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.598661</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>167</td>\n",
       "      <td>29</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.406860</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>197</td>\n",
       "      <td>15</td>\n",
       "      <td>0.361607</td>\n",
       "      <td>0.280142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.320745</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>227</td>\n",
       "      <td>15</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.329787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>166</td>\n",
       "      <td>6</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.698127</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>231</td>\n",
       "      <td>11</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.329787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.701033</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>68</td>\n",
       "      <td>23</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.315603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.771253</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>139</td>\n",
       "      <td>22</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.269504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.272479</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>257</td>\n",
       "      <td>15</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.347518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.260813</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>141</td>\n",
       "      <td>13</td>\n",
       "      <td>0.379464</td>\n",
       "      <td>0.351064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.707345</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>283</td>\n",
       "      <td>13</td>\n",
       "      <td>0.379464</td>\n",
       "      <td>0.393617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.201360</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>182</td>\n",
       "      <td>11</td>\n",
       "      <td>0.379464</td>\n",
       "      <td>0.308511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.361995</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>236</td>\n",
       "      <td>27</td>\n",
       "      <td>0.379464</td>\n",
       "      <td>0.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.495156</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>196</td>\n",
       "      <td>16</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.301418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.570139</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>224</td>\n",
       "      <td>6</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.297872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.697269</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>251</td>\n",
       "      <td>9</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.301418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.432775</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>89</td>\n",
       "      <td>16</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.613603</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>248</td>\n",
       "      <td>5</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.312057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.563580</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>240</td>\n",
       "      <td>5</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.287234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.487891</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>167</td>\n",
       "      <td>11</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.308511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.353662</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>193</td>\n",
       "      <td>22</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.347518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.501809</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>206</td>\n",
       "      <td>19</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.308511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.220120</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>245</td>\n",
       "      <td>13</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.326241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.467118</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>186</td>\n",
       "      <td>12</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.622787</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>77</td>\n",
       "      <td>11</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.304965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.261011</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>235</td>\n",
       "      <td>5</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.315603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572285</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.312057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.649477</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>92</td>\n",
       "      <td>6</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.308511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.656908</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.555510</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.465648</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>186</td>\n",
       "      <td>6</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>0.283688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.311690</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>0.301418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.458693</td>\n",
       "      <td>0.009508</td>\n",
       "      <td>65</td>\n",
       "      <td>23</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.312057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.492003</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>223</td>\n",
       "      <td>14</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.672667</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>142</td>\n",
       "      <td>9</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.283688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.708803</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>130</td>\n",
       "      <td>16</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.368794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.463345</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.326241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.784811</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>165</td>\n",
       "      <td>19</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.361702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.637266</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>130</td>\n",
       "      <td>20</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.308511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.445660</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>73</td>\n",
       "      <td>28</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.347518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.656613</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>175</td>\n",
       "      <td>9</td>\n",
       "      <td>0.419643</td>\n",
       "      <td>0.358156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.706806</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>208</td>\n",
       "      <td>18</td>\n",
       "      <td>0.419643</td>\n",
       "      <td>0.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.430194</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>238</td>\n",
       "      <td>24</td>\n",
       "      <td>0.424107</td>\n",
       "      <td>0.312057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.300320</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>121</td>\n",
       "      <td>28</td>\n",
       "      <td>0.424107</td>\n",
       "      <td>0.308511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.613198</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>157</td>\n",
       "      <td>11</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.240222</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.280142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.433356</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>138</td>\n",
       "      <td>5</td>\n",
       "      <td>0.433036</td>\n",
       "      <td>0.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.310587</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>202</td>\n",
       "      <td>21</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.322695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.794352</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>184</td>\n",
       "      <td>8</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>0.390071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.214287</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>233</td>\n",
       "      <td>17</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.304965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.388336</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>255</td>\n",
       "      <td>20</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.294326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    droprate  learning_rates  hidden_dim  v_dim  dev_accuracy  test_accuracy\n",
       "55  0.216915        0.006588          97     23      0.303571       0.336879\n",
       "52  0.637862        0.003939         240     15      0.325893       0.280142\n",
       "51  0.494909        0.001578         149     28      0.334821       0.336879\n",
       "56  0.350545        0.000482         264     29      0.339286       0.297872\n",
       "35  0.638593        0.004840         166     25      0.343750       0.280142\n",
       "41  0.531746        0.004108         281     19      0.348214       0.368794\n",
       "36  0.356340        0.006259         231     17      0.348214       0.354610\n",
       "17  0.532029        0.003339         129     27      0.352679       0.322695\n",
       "31  0.579366        0.004421         152      6      0.352679       0.368794\n",
       "6   0.498583        0.002622         211     18      0.357143       0.297872\n",
       "50  0.598661        0.002638         167     29      0.357143       0.340426\n",
       "22  0.406860        0.001219         197     15      0.361607       0.280142\n",
       "53  0.320745        0.000648         227     15      0.366071       0.329787\n",
       "13  0.264220        0.007224         166      6      0.366071       0.333333\n",
       "57  0.698127        0.004388         231     11      0.366071       0.329787\n",
       "7   0.701033        0.002479          68     23      0.366071       0.315603\n",
       "27  0.771253        0.001313         139     22      0.375000       0.269504\n",
       "46  0.272479        0.001346         257     15      0.375000       0.347518\n",
       "45  0.260813        0.000284         141     13      0.379464       0.351064\n",
       "49  0.707345        0.002124         283     13      0.379464       0.393617\n",
       "34  0.201360        0.001909         182     11      0.379464       0.308511\n",
       "16  0.361995        0.000257         236     27      0.379464       0.340426\n",
       "15  0.495156        0.006587         196     16      0.383929       0.301418\n",
       "12  0.570139        0.000216         224      6      0.383929       0.297872\n",
       "9   0.697269        0.002428         251      9      0.383929       0.301418\n",
       "19  0.432775        0.001187          89     16      0.388393       0.340426\n",
       "38  0.613603        0.000348         248      5      0.388393       0.312057\n",
       "2   0.563580        0.002183         240      5      0.388393       0.287234\n",
       "20  0.487891        0.000184         167     11      0.388393       0.308511\n",
       "59  0.353662        0.001197         193     22      0.388393       0.347518\n",
       "14  0.501809        0.000295         206     19      0.392857       0.308511\n",
       "11  0.220120        0.001521         245     13      0.392857       0.326241\n",
       "39  0.467118        0.002523         186     12      0.392857       0.333333\n",
       "3   0.622787        0.002692          77     11      0.392857       0.304965\n",
       "48  0.261011        0.000422         235      5      0.397321       0.315603\n",
       "0   0.572285        0.002560          67     22      0.397321       0.312057\n",
       "18  0.649477        0.000296          92      6      0.397321       0.308511\n",
       "33  0.656908        0.008982         139     26      0.397321       0.333333\n",
       "58  0.555510        0.000143         102      7      0.401786       0.333333\n",
       "21  0.465648        0.001818         186      6      0.401786       0.283688\n",
       "44  0.311690        0.005945         152      9      0.401786       0.301418\n",
       "37  0.458693        0.009508          65     23      0.406250       0.312057\n",
       "32  0.492003        0.000196         223     14      0.410714       0.333333\n",
       "1   0.672667        0.001888         142      9      0.410714       0.283688\n",
       "23  0.708803        0.000182         130     16      0.410714       0.368794\n",
       "25  0.463345        0.000123          74     15      0.410714       0.326241\n",
       "24  0.784811        0.008769         165     19      0.415179       0.361702\n",
       "29  0.637266        0.000151         130     20      0.415179       0.308511\n",
       "40  0.445660        0.000278          73     28      0.415179       0.347518\n",
       "10  0.656613        0.000506         175      9      0.419643       0.358156\n",
       "5   0.706806        0.000155         208     18      0.419643       0.340426\n",
       "28  0.430194        0.000147         238     24      0.424107       0.312057\n",
       "54  0.300320        0.000110         121     28      0.424107       0.308511\n",
       "43  0.613198        0.000131         157     11      0.428571       0.340426\n",
       "26  0.240222        0.000114         149      5      0.428571       0.280142\n",
       "47  0.433356        0.000145         138      5      0.433036       0.340426\n",
       "30  0.310587        0.000109         202     21      0.437500       0.322695\n",
       "8   0.794352        0.000172         184      8      0.450893       0.390071\n",
       "42  0.214287        0.000268         233     17      0.455357       0.304965\n",
       "4   0.388336        0.000178         255     20      0.455357       0.294326"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"dev_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Baseline.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
